# Emojify!

æ¬¢è¿æ¥åˆ°ç¬¬äºŒå‘¨çš„ç¬¬äºŒä¸ªä½œä¸šã€‚æ‚¨å°†ä½¿ç”¨å•è¯å‘é‡è¡¨ç¤ºæ¥æ„å»ºä¸€ä¸ªè¡¨æƒ…ç¬¦å·ã€‚

ä½ æ›¾ç»æƒ³è¿‡è®©ä½ çš„çŸ­ä¿¡æ›´æœ‰è¡¨ç°åŠ›å—?ä½ çš„è¡¨æƒ…åº”ç”¨ä¼šå¸®ä½ åšåˆ°è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥ä¸è¦å†™â€œæ­å–œä½ å‡èŒäº†!â€æˆ‘ä»¬å–æ¯å’–å•¡èŠèŠå¤©å§ã€‚è¿™ä¸ªè¡¨æƒ…ç¬¦å·å¯ä»¥è‡ªåŠ¨è½¬æ¢æˆâ€œæ­å–œä½ å‡èŒäº†!â€ğŸ‘è®©å–å’–å•¡èŠå¤©ã€‚â˜•ï¸çˆ±ä½ !â¤ï¸â€

æ‚¨å°†å®ç°ä¸€ä¸ªæ¨¡å‹,è¾“å…¥ä¸€ä¸ªå¥å­(å¦‚â€œä»Šæ™šæˆ‘ä»¬å»çœ‹æ£’çƒæ¯”èµ›!â€),æ‰¾åˆ°ä½¿ç”¨åœ¨è¿™ä¸ªå¥å­ä¸Šæœ€åˆé€‚çš„emoji(âš¾ï¸)ã€‚åœ¨è®¸å¤šemojiæ¥å£ä¸­,æ‚¨éœ€è¦è®°ä½,â¤ï¸æ˜¯â€œå¿ƒâ€è€Œä¸æ˜¯â€œçˆ±â€çš„è±¡å¾ç¬¦å·ã€‚ä½†ä½¿ç”¨è¯å‘é‡,ä½ ä¼šå‘ç°å³ä½¿ä½ çš„è®­ç»ƒé›†ä»…èƒ½æ˜¾å¼åœ°ä½¿ä¸€äº›å•è¯ä¸ç‰¹å®šemojiç›¸å…³,ä½ çš„ç®—æ³•å¯ä»¥æ¨å¹¿å’Œæµ‹è¯•é›†çš„å…³è”è¯è¯­ç›¸åŒçš„emojiå³ä½¿è¿™äº›è¯ç”šè‡³ä¸å‡ºç°åœ¨è®­ç»ƒé›†ã€‚è¿™å…è®¸æ‚¨ç”šè‡³ç”¨ä¸€ä¸ªå°è®­ç»ƒé›†å°±å¯ä»¥æ„å»ºä¸€ä¸ªç²¾ç¡®çš„åˆ†ç±»å™¨ä»å¥å­åˆ°emojiçš„æ˜ å°„ã€‚

åœ¨è¿™ä¸ªç»ƒä¹ ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨è¯åµŒå…¥ä»ä¸€ä¸ªåŸºçº¿æ¨¡å‹ (Emojifier-V1)å¼€å§‹ï¼Œç„¶åæ„å»ºä¸€ä¸ªæ›´å¤æ‚çš„æ¨¡å‹(Emojifier-V2) ï¼Œè¯¥æ¨¡å‹è¿›ä¸€æ­¥åˆå¹¶äº†ä¸€ä¸ªLSTMã€‚

## å¯¼åŒ…

```python
import numpy as np
from emo_utils import *
import emoji
import matplotlib.pyplot as plt

%matplotlib inline
```

## åŸºçº¿æ¨¡å‹: Emojifier-V1

### æ•°æ®é›†EMOJISET

è®©æˆ‘ä»¬ä»æ„å»ºä¸€ä¸ªç®€å•çš„åŸºçº¿åˆ†ç±»å™¨å¼€å§‹ã€‚

æ‚¨æœ‰ä¸€ä¸ªå°æ•°æ®é›†(X, Y)ï¼Œå…¶ä¸­:

- XåŒ…å«127ä¸ªå¥å­(å­—ç¬¦ä¸²)
- YåŒ…å«ä¸€ä¸ª0åˆ°4ä¹‹é—´çš„æ•´æ•°æ ‡ç­¾ï¼Œå¯¹åº”æ¯å¥è¯çš„è¡¨æƒ…ç¬¦å·

![](img/a7.jpg)

è®©æˆ‘ä»¬ä½¿ç”¨ä¸‹é¢çš„ä»£ç åŠ è½½æ•°æ®é›†ã€‚æˆ‘ä»¬å°†æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒ(127ä¸ªä¾‹å­)å’Œæµ‹è¯•(56ä¸ªä¾‹å­)ã€‚

```python
X_train, Y_train = read_csv('data/train_emoji.csv')
X_test, Y_test = read_csv('data/tesss.csv')
```

#### æµ‹è¯•

```python
maxLen = len(max(X_train, key=len).split())
print(maxLen)
```

#### ç»“æœ

```
10
```



#### æŸ¥çœ‹æ•°æ®

```python
index = 1
print(X_train[index], label_to_emoji(Y_train[index]))
```

#### ç»“æœ

```
I am proud of your achievements ğŸ˜„
```

### Emojifier-V1æ¦‚è¿°

åœ¨æœ¬éƒ¨åˆ†ä¸­ï¼Œæ‚¨å°†å®ç°ä¸€ä¸ªåä¸ºâ€œEmojifier -V1â€çš„åŸºçº¿æ¨¡å‹ã€‚

![](img/a6.jpg)

æ¨¡å‹çš„è¾“å…¥æ˜¯ä¸€ä¸ªä¸å¥å­ç›¸å¯¹åº”çš„å­—ç¬¦ä¸²(æ¯”å¦‚"I love you")ã€‚åœ¨ä»£ç ä¸­ï¼Œè¾“å‡ºå°†æ˜¯ä¸€ä¸ªshapeä¸º(1,5)çš„æ¦‚ç‡å‘é‡ï¼Œç„¶åä¼ å…¥ä¸€ä¸ªargmaxå±‚ä»¥æå–æœ€å¯èƒ½çš„emojiç¬¦å·è¾“å‡ºçš„ç´¢å¼•ã€‚

ä¸ºäº†è®©æˆ‘ä»¬çš„æ ‡ç­¾æ ¼å¼è´´åˆsoftmaxåˆ†ç±»å™¨è®­ç»ƒï¼Œè¦è®©$Y$ä»ç›®å‰çš„shape$(m, 1)$è½¬æ¢ä¸ºâ€œone-hotçš„è¡¨ç¤ºâ€$(m, 5)$ï¼Œå…¶ä¸­æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ªç»™å®šæ ·æœ¬æ ‡ç­¾çš„one-hotå‘é‡ã€ä½ å¯ä»¥ä½¿ç”¨ä¸‹ä¸€ä¸ªä»£ç snipperæ¥è¿™ä¹ˆåšã€‘ã€‚

è¿™é‡Œï¼Œ` Y_oh `åœ¨å˜é‡å` Y_oh_train `å’Œ` Y_oh_test `ä¸­ä»£è¡¨" Y-one-hot":

```python
Y_oh_train = convert_to_one_hot(Y_train, C = 5)
Y_oh_test = convert_to_one_hot(Y_test, C = 5)
```

è®©æˆ‘ä»¬çœ‹çœ‹` convert_to_one_hot()`åšäº†ä»€ä¹ˆã€‚æ‚¨å¯ä»¥éšæ„æ›´æ”¹` index`ä»¥è¾“å‡ºä¸åŒçš„å€¼ã€‚

```python
index = 50
print(Y_train[index], "is converted into one hot", Y_oh_train[index])
```

ç»“æœ

```
0 is converted into one hot [1. 0. 0. 0. 0.]
```

ç°åœ¨ï¼Œæ‰€æœ‰æ•°æ®éƒ½å‡†å¤‡å¥½äº†ï¼Œå¯ä»¥è¾“å…¥Emojify-V1æ¨¡å‹ã€‚è®©æˆ‘ä»¬æ¥å®ç°è¿™ä¸ªæ¨¡å‹!

### å®ç°Emojifier-V1

#### GloVeåµŒå…¥

ç¬¬ä¸€æ­¥æ˜¯å°†è¾“å…¥çš„å¥å­è½¬æ¢ä¸ºå•è¯å‘é‡è¡¨ç¤ºï¼Œç„¶åå°†å®ƒä»¬ä¸€èµ·å¹³å‡ã€‚ä¸å‰é¢çš„ç»ƒä¹ ç±»ä¼¼ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„50ç»´GloVeåµŒå…¥ã€‚è¿è¡Œä»¥ä¸‹å•å…ƒæ ¼ä»¥åŠ è½½`word_to_vec_map`ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰å‘é‡è¡¨ç¤ºã€‚

```python
word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')
```

ä½ å·²ç»åŠ è½½äº†ï¼š

- `word_to_index`: ä»å•è¯åˆ°è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•çš„å­—å…¸æ˜ å°„(400,001ä¸ªå•è¯ï¼Œæœ‰æ•ˆç´¢å¼•ä»0åˆ°400,000ä¸ªå•è¯ä¸ç­‰)
- `index_to_word`: å­—å…¸ä»ç´¢å¼•æ˜ å°„åˆ°è¯æ±‡è¡¨ä¸­å¯¹åº”çš„å•è¯
- `word_to_vec_map`: å­—å…¸æ˜ å°„å•è¯åˆ°ä»–ä»¬çš„GloVeå‘é‡è¡¨ç¤ºã€‚

##### æµ‹è¯•

è¿è¡Œä¸‹é¢çš„å•å…ƒæ ¼æ£€æŸ¥å®ƒæ˜¯å¦å·¥ä½œã€‚

```python
word = "cucumber"
index = 289846
print("the index of", word, "in the vocabulary is", word_to_index[word])
print("the", str(index) + "th word in the vocabulary is", index_to_word[index])
```

##### ç»“æœ

```
the index of cucumber in the vocabulary is 113317
the 289846th word in the vocabulary is potatos
```



#### å¹³å‡å¥å­

```python
# GRADED FUNCTION: sentence_to_avg

def sentence_to_avg(sentence, word_to_vec_map):
    """
    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word
    and averages its value into a single vector encoding the meaning of the sentence.
    
    Arguments:
    sentence -- string, one training example from X
    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation
    
    Returns:
    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)
    """
    
    ### START CODE HERE ###
    # æ­¥éª¤ 1: æŠŠå¥å­åˆ†æˆä¸€ç»„å°å†™çš„å•è¯ (â‰ˆ 1 line)
    words = sentence.lower().split()

    # é›¶åˆå§‹åŒ–å¹³å‡å•è¯å‘é‡ï¼Œåº”è¯¥ä¸ä½ çš„å•è¯å‘é‡å…·æœ‰ç›¸åŒçš„shpaeã€‚
    avg = np.zeros(word_to_vec_map[words[0]].shape)
    
    # æ­¥éª¤ 2: å¹³å‡å•è¯å‘é‡ã€‚ä½ å¯ä»¥å¾ªç¯éå†"words"åˆ—è¡¨ä¸­çš„å•è¯ã€‚
    for w in words:
        avg += word_to_vec_map[w]
    avg = avg / len(words)
    
    ### END CODE HERE ###
    
    return avg
```

##### æµ‹è¯•

```python
avg = sentence_to_avg("Morrocan couscous is my favorite dish", word_to_vec_map)
print("avg = ", avg)
```

##### ç»“æœ

```
avg =  [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983
 -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867
  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767
  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061
  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265
  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925
 -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333
 -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433
  0.1445417   0.09808667]
```



#### æ¨¡å‹

ç°åœ¨ï¼Œæ‚¨å·²ç»å®Œæˆäº†å®ç°`model()`å‡½æ•°çš„æ‰€æœ‰å·¥ä½œã€‚åœ¨ä½¿ç”¨`sentence_to_avg() `ä¹‹åï¼Œæ‚¨éœ€è¦é€šè¿‡æ­£å‘ä¼ æ’­ä¼ é€’å¹³å‡å€¼ï¼Œè®¡ç®—æˆæœ¬ï¼Œç„¶ååå‘ä¼ æ’­ä»¥æ›´æ–°softmaxçš„å‚æ•°ã€‚

å®ç°`model() `å‡½æ•°ã€‚å‡è®¾$Yoh$ ("Y one hot")æ˜¯è¾“å‡ºæ ‡ç­¾çš„one-hotç¼–ç ï¼Œåœ¨å‰ä¼ è¿‡ç¨‹ä¸­éœ€è¦å®ç°çš„æ–¹ç¨‹å’Œè®¡ç®—äº¤å‰ç†µæŸå¤±æ˜¯:
$$
z^{(i)} = W . avg^{(i)} + b
$$
$$
a^{(i)} = softmax(z^{(i)})
$$
$$
\mathcal{L}^{(i)} = - \sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)
$$

æœ‰å¯èƒ½ä¼šæœ‰ä¸€ä¸ªæ›´æœ‰æ•ˆçš„å‘é‡åŒ–å®ç°ã€‚ä½†æ˜¯ç”±äºæˆ‘ä»¬ä½¿ç”¨forå¾ªç¯å°†è¯­å¥ä¸€æ¬¡è½¬æ¢ä¸º$avg^{(i)}$è¡¨ç¤ºï¼Œæ‰€ä»¥è¿™æ¬¡æˆ‘ä»¬å°±ä¸ç”¨è´¹å¿ƒäº†ã€‚

```python
# GRADED FUNCTION: model

def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):
    """
    Model to train word vector representations in numpy.
    
    Arguments:
    X -- input data, numpy array of sentences as strings, of shape (m, 1)
    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)
    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation
    learning_rate -- learning_rate for the stochastic gradient descent algorithm
    num_iterations -- number of iterations
    
    Returns:
    pred -- vector of predictions, numpy-array of shape (m, 1)
    W -- weight matrix of the softmax layer, of shape (n_y, n_h)
    b -- bias of the softmax layer, of shape (n_y,)
    """
    
    np.random.seed(1)

    # å®šä¹‰è®­ç»ƒæ ·æœ¬çš„æ•°é‡
    # è®­ç»ƒæ ·æœ¬çš„æ•°é‡
    m = Y.shape[0]
    # ç±»åˆ«æ•°é‡
    n_y = 5             
    # GloVeå‘é‡ç»´åº¦
    n_h = 50                               
    
    # ä½¿ç”¨Xavieråˆå§‹åŒ–å‚æ•°
    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)
    b = np.zeros((n_y,))
    
    # ä½¿ç”¨n_yç§ç±»å°†Yè½¬æ¢ä¸ºY_onehot
    Y_oh = convert_to_one_hot(Y, C = n_y) 
    
    # ä¼˜åŒ–å¾ªç¯
    # å¾ªç¯çš„è¿­ä»£æ¬¡æ•°
    for t in range(num_iterations):
        # å¾ªç¯çš„è®­ç»ƒæ ·æœ¬
        for i in range(m):
            
            ### START CODE HERE ### (â‰ˆ 4 lines of code)
            # å¯¹ç¬¬iä¸ªè®­ç»ƒæ ·æœ¬ä¸­çš„å•è¯å‘é‡è¿›è¡Œå¹³å‡
            avg = sentence_to_avg(X[i], word_to_vec_map)

            # é€šè¿‡softmaxå±‚æ­£å‘ä¼ æ’­avg
            z = np.matmul(W, avg) + b
            a = softmax(z)

            # ä½¿ç”¨ç¬¬iä¸ªè®­ç»ƒæ ‡ç­¾çš„ä¸€ä¸ªone-hotè¡¨ç¤ºå’Œâ€œAâ€(softmaxçš„è¾“å‡º)å…±åŒè®¡ç®—cost
            cost = - np.sum(Y_oh[i] * np.log(a))
            ### END CODE HERE ###
            
            # è®¡ç®—æ¢¯åº¦
            dz = a - Y_oh[i]
            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))
            db = dz

            # ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°
            W = W - learning_rate * dW
            b = b - learning_rate * db
        
        if t % 100 == 0:
            print("Epoch: " + str(t) + " --- cost = " + str(cost))
            pred = predict(X, Y, W, b, word_to_vec_map)

    return pred, W, b
```

##### æµ‹è¯•

```python
print(X_train.shape)
print(Y_train.shape)
print(np.eye(5)[Y_train.reshape(-1)].shape)
print(X_train[0])
print(type(X_train))
Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])
print(Y.shape)

X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',
 'Lets go party and drinks','Congrats on the new job','Congratulations',
 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',
 'You totally deserve this prize', 'Let us go play football',
 'Are you down for football this afternoon', 'Work hard play harder',
 'It is suprising how people can be dumb sometimes',
 'I am very disappointed','It is the best day in my life',
 'I think I will end up alone','My life is so boring','Good job',
 'Great so awesome'])

print(X.shape)
print(np.eye(5)[Y_train.reshape(-1)].shape)
print(type(X_train))
```

##### ç»“æœ

```
(132,)
(132,)
(132, 5)
never talk to me again
<class 'numpy.ndarray'>
(20,)
(20,)
(132, 5)
<class 'numpy.ndarray'>
```

#### è®­ç»ƒæ¨¡å‹

è¿è¡Œä¸‹ä¸€ä¸ªå•å…ƒæ¥è®­ç»ƒä½ çš„æ¨¡å‹å¹¶å­¦ä¹ softmaxå‚æ•°(W,b)ã€‚

```python
pred, W, b = model(X_train, Y_train, word_to_vec_map)
# print(pred)
```

##### ç»“æœ

```
Epoch: 0 --- cost = 1.9520498812810072
Accuracy: 0.3484848484848485
Epoch: 100 --- cost = 0.07971818726014807
Accuracy: 0.9318181818181818
Epoch: 200 --- cost = 0.04456369243681402
Accuracy: 0.9545454545454546
Epoch: 300 --- cost = 0.03432267378786059
Accuracy: 0.9696969696969697
```

å¤ªæ£’äº†!ä½ çš„æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šæœ‰ç›¸å½“é«˜çš„ç²¾ç¡®åº¦ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å®ƒåœ¨æµ‹è¯•é›†ä¸Šæ˜¯æ€æ ·çš„ã€‚

### æ£€éªŒæµ‹è¯•é›†æ€§èƒ½

#### æµ‹è¯•

```python
print("Training set:")
pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)
print('Test set:')
pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)
```

#### ç»“æœ

```
Training set:
Accuracy: 0.9772727272727273
Test set:
Accuracy: 0.8571428571428571
```

å‡è®¾æœ‰5ç±»ï¼ŒéšæœºçŒœæµ‹çš„å‡†ç¡®ç‡ä¸º20%ã€‚åœ¨åªè®­ç»ƒäº†127ä¸ªç¤ºä¾‹ä¹‹åï¼Œè¿™å·²ç»æ˜¯ç›¸å½“ä¸é”™çš„æ€§èƒ½äº†ã€‚

#### ä¸åœ¨è®­ç»ƒé›†çš„å•è¯æµ‹è¯•

ç®—æ³•åœ¨è®­ç»ƒé›†,çœ‹åˆ°è¿™å¥è¯â€œ*I love you*â€æ ‡ç­¾â¤ï¸ã€‚ä½ å¯ä»¥æ£€æŸ¥ï¼Œä½†æ˜¯å•è¯â€œ*adore*â€æ²¡æœ‰å‡ºç°åœ¨è®­ç»ƒé›†ä¸­ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚æœä½ å†™"*I adore You*"ä¼šå‘ç”Ÿä»€ä¹ˆã€‚

##### æµ‹è¯•

```python
X_my_sentences = np.array(["i adore you", "i love you", "funny lol", "lets play with a ball", "food is ready", "not feeling happy"])
Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])

pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)
print_predictions(X_my_sentences, pred)
```

##### ç»“æœ

```
Accuracy: 0.8333333333333334

i adore you â¤ï¸
i love you â¤ï¸
funny lol ğŸ˜„
lets play with a ball âš¾
food is ready ğŸ´
not feeling happy ğŸ˜„
```



å¤ªç¥å¥‡äº†!å› ä¸º*adore*å’Œ*love*æœ‰ç›¸ä¼¼çš„åµŒå…¥ï¼Œç®—æ³•æ­£ç¡®åœ°æ¦‚æ‹¬äº†ä¸€ä¸ªè¯ï¼Œç”šè‡³æ˜¯å®ƒä»¥å‰ä»æœªè§è¿‡çš„è¯ã€‚åƒ*heart*ï¼Œ *dear*ï¼Œ *beloved*æˆ–*adore*è¿™æ ·çš„è¯éƒ½æœ‰ç±»ä¼¼äº*love*çš„åµŒå…¥å‘é‡ï¼Œæ‰€ä»¥ä¹Ÿå¯ä»¥è¿™æ ·åšâ€”â€”éšæ„ä¿®æ”¹ä¸Šé¢çš„è¾“å…¥ï¼Œå°è¯•å„ç§ä¸åŒçš„è¾“å…¥å¥å­ã€‚å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„å‘¢?

è¯·æ³¨æ„ï¼Œâ€œä¸å¼€å¿ƒâ€å¹¶ä¸æ˜¯æ­£ç¡®çš„ã€‚è¿™ç§ç®—æ³•å¿½ç•¥äº†å•è¯æ’åºï¼Œå› æ­¤ä¸èƒ½å¾ˆå¥½åœ°ç†è§£åƒâ€œnot happyâ€è¿™æ ·çš„çŸ­è¯­ã€‚

### æ··æ·†çŸ©é˜µ

æ‰“å°**æ··æ·†çŸ©é˜µï¼ˆconfusion matrixï¼‰**è¿˜å¯ä»¥å¸®åŠ©ç†è§£å“ªäº›ç±»å¯¹æ‚¨çš„æ¨¡å‹æ¥è¯´æ›´å›°éš¾ã€‚æ··æ·†çŸ©é˜µæ˜¾ç¤ºäº†ä¸€ä¸ªç¤ºä¾‹çš„æ ‡ç­¾æ˜¯ä¸€ä¸ªç±»(â€œå®é™…çš„â€ç±»)è¢«ç®—æ³•ç”¨å¦ä¸€ä¸ªç±»(â€œé¢„æµ‹çš„â€ç±»)é”™è¯¯æ ‡è®°çš„é¢‘ç‡ã€‚

#### æµ‹è¯•

```python
print(Y_test.shape)
print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))
print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))
plot_confusion_matrix(Y_test, pred_test)
```

#### ç»“æœ

```
(56,)
           â¤ï¸    âš¾    ğŸ˜„    ğŸ˜   ğŸ´
Predicted  0.0  1.0  2.0  3.0  4.0  All
Actual                                 
0            6    0    0    1    0    7
1            0    8    0    0    0    8
2            2    0   16    0    0   18
3            1    1    2   12    0   16
4            0    0    1    0    6    7
All          9    9   19   13    6   56
```

![](img/a5.jpg)

**ä½ ä»è¿™éƒ¨åˆ†åº”è¯¥è®°ä½çš„**:

- å³ä½¿æœ‰127ä¸ªè®­ç»ƒä¾‹å­ï¼Œä½ å¯ä»¥å¾—åˆ°ä¸€ä¸ªç›¸å½“å¥½çš„æ¨¡å‹æ¥ä½¿ç”¨è¡¨æƒ…ç¬¦å·ã€‚è¿™ä¸»è¦æ˜¯ç”±äºæ³›åŒ–èƒ½åŠ›è¯å‘é‡ã€‚
- Emojify-V1ä¼šåœ¨*â€œThis movie is not good and not enjoyableâ€*è¿™æ ·çš„å¥å­ä¸­è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒ**ä¸ç†è§£å•è¯çš„ç»„åˆ**â€”â€”å®ƒåªæ˜¯å°†æ‰€æœ‰å•è¯çš„åµŒå…¥å‘é‡**å¹³å‡åœ¨ä¸€èµ·**ï¼Œè€Œ**æ²¡æœ‰æ³¨æ„å•è¯çš„é¡ºåº**ã€‚åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œæ‚¨å°†æ„å»ºä¸€ä¸ªæ›´å¥½çš„ç®—æ³•ã€‚

## ä½¿ç”¨Kerasä¸­çš„LSTM: Emojifier-V2

è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªæ¥å—è¾“å…¥å•è¯åºåˆ—çš„LSTMæ¨¡å‹ã€‚è¿™ä¸ªæ¨¡å‹å°†èƒ½å¤Ÿè€ƒè™‘åˆ°å­—çš„é¡ºåºã€‚Emojifier - V2å°†ç»§ç»­ä½¿ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„åµŒå…¥å•è¯æ¥è¡¨ç¤ºå•è¯ï¼Œä½†æ˜¯ä¼šå°†å®ƒä»¬è¾“å…¥åˆ°ä¸€ä¸ªLSTMä¸­ï¼Œåè€…çš„å·¥ä½œå°±æ˜¯é¢„æµ‹å‡ºæœ€åˆé€‚çš„emojiã€‚

### å¯¼åŒ…

```python
import numpy as np
np.random.seed(0)
from keras.models import Model
from keras.layers import Dense, Input, Dropout, LSTM, Activation
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.initializers import glorot_uniform
np.random.seed(1)
```

### æ¨¡å‹æ¦‚è¿°

![](img/a8.jpg)

### Keras å’Œ mini-batch

åœ¨è¿™ä¸ªç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨mini-batchæ¥è®­ç»ƒKerasã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¡†æ¶éƒ½è¦æ±‚åŒä¸€ä¸ªmini-batchä¸­çš„æ‰€æœ‰åºåˆ—å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚è¿™å°±æ˜¯çŸ¢é‡åŒ–å·¥ä½œçš„åŸå› :å¦‚æœæ‚¨æœ‰ä¸€ä¸ª3ä¸ªè¯çš„å¥å­å’Œä¸€ä¸ª4ä¸ªè¯çš„å¥å­ï¼Œé‚£ä¹ˆå®ƒä»¬éœ€è¦çš„è®¡ç®—æ˜¯ä¸åŒçš„(ä¸€ä¸ªéœ€è¦LSTMçš„3ä¸ªæ­¥éª¤ï¼Œä¸€ä¸ªéœ€è¦4ä¸ªæ­¥éª¤)ï¼Œæ‰€ä»¥ä¸å¯èƒ½åŒæ—¶å®Œæˆå®ƒä»¬ã€‚

å¸¸è§çš„è§£å†³æ–¹æ³•æ˜¯ä½¿ç”¨paddingå¡«å……ã€‚

å…·ä½“æ¥è¯´ï¼Œè®¾ç½®ä¸€ä¸ªæœ€å¤§åºåˆ—é•¿åº¦ï¼Œå¹¶å¡«å……æ‰€æœ‰åºåˆ—åˆ°ç›¸åŒçš„é•¿åº¦ã€‚ä¾‹å¦‚ï¼Œæœ€å¤§åºåˆ—é•¿åº¦ä¸º20æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨â€œ0â€å¡«å……æ¯ä¸ªå¥å­ï¼Œè¿™æ ·æ¯ä¸ªè¾“å…¥å¥å­çš„é•¿åº¦ä¸º20ã€‚å› æ­¤ï¼Œå¥å­"i love you"å°†è¢«è¡¨ç¤ºä¸º$(e_{i}ï¼Œ e_{love}ï¼Œ e_{you}ï¼Œ \vec{0}ï¼Œ \vec{0}ï¼Œ \ldotsï¼Œ \vec{0})$ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä»»ä½•è¶…è¿‡20ä¸ªå•è¯çš„å¥å­éƒ½å¿…é¡»è¢«æˆªæ–­ã€‚é€‰æ‹©æœ€å¤§åºåˆ—é•¿åº¦çš„ä¸€ä¸ªç®€å•æ–¹æ³•å°±æ˜¯é€‰æ‹©è®­ç»ƒé›†ä¸­æœ€é•¿å¥å­çš„é•¿åº¦ã€‚

### åµŒå…¥å±‚

Keraså°†åµŒå…¥çŸ©é˜µè¡¨ç¤ºä¸ºâ€œå±‚â€ï¼Œå°†æ­£æ•´æ•°(è¯å¯¹åº”çš„ç´¢å¼•)æ˜ å°„ä¸ºå›ºå®šå¤§å°çš„**å¯†é›†å‘é‡**(åµŒå…¥å‘é‡)ã€‚å®ƒå¯ä»¥é€šè¿‡**é¢„å…ˆè®­ç»ƒå¥½çš„åµŒå…¥æ¥è®­ç»ƒæˆ–åˆå§‹åŒ–**ã€‚åœ¨æœ¬éƒ¨åˆ†ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•åœ¨Kerasä¸­åˆ›å»º[Embedding()](https://keras.io/layers/embeddings/)å±‚ï¼Œå¹¶ç”¨ä¹‹å‰åŠ è½½çš„GloVe 50ç»´å‘é‡åˆå§‹åŒ–å®ƒã€‚å› ä¸ºæˆ‘ä»¬çš„è®­ç»ƒé›†éå¸¸å°ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸ä¼šæ›´æ–°è¯åµŒå…¥ï¼Œè€Œæ˜¯å°†å…¶å€¼ä¿æŒä¸å˜ã€‚ä½†æ˜¯åœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬å°†å‘æ‚¨å±•ç¤ºKerasæ˜¯å¦‚ä½•å…è®¸æ‚¨è®­ç»ƒæˆ–ä¿®å¤è¿™ä¸€å±‚çš„ã€‚

`Embedding()`å±‚é‡‡ç”¨ä¸€ä¸ªå¤§å°ä¸ºæ•´æ•°çš„çŸ©é˜µ(batchå¤§å°ï¼Œæœ€å¤§è¾“å…¥é•¿åº¦)ä½œä¸ºè¾“å…¥ã€‚è¿™å¯¹åº”äºè½¬æ¢ä¸ºç´¢å¼•(æ•´æ•°)åˆ—è¡¨çš„å¥å­ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](img/a9.jpg)

åµŒå…¥å±‚ã€‚è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†é€šè¿‡åµŒå…¥å±‚ä¼ æ’­ä¸¤ä¸ªç¤ºä¾‹ã€‚ä¸¤è€…éƒ½è¢«é›¶å¡«å……åˆ°` max_len=5 `çš„é•¿åº¦ã€‚è¡¨ç¤ºçš„æœ€åä¸€ä¸ªç»´åº¦æ˜¯`(2,max_len,50) `ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„è¯åµŒå…¥æ˜¯50ç»´çš„ã€‚

è¾“å…¥çš„æœ€å¤§æ•´æ•°(å³å•è¯ç´¢å¼•)ä¸åº”å¤§äºè¯æ±‡è¡¨çš„å¤§å°ã€‚è¯¥å±‚è¾“å‡ºä¸€ä¸ªå½¢çŠ¶æ•°ç»„(æ‰¹å¤§å°ï¼Œæœ€å¤§è¾“å…¥é•¿åº¦ï¼Œå­—å‘é‡çš„å°ºå¯¸)ã€‚

ç¬¬ä¸€æ­¥æ˜¯å°†æ‰€æœ‰çš„è®­ç»ƒå¥å­è½¬æ¢æˆç´¢å¼•åˆ—è¡¨ï¼Œç„¶åå¯¹æ‰€æœ‰è¿™äº›åˆ—è¡¨è¿›è¡Œé›¶å¡«å……ï¼Œä½¿å®ƒä»¬çš„é•¿åº¦ç­‰äºæœ€é•¿å¥å­çš„é•¿åº¦ã€‚

```python
# GRADED FUNCTION: sentences_to_indices

def sentences_to_indices(X, word_to_index, max_len):
    """
    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.
    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). 
    
    Arguments:
    X -- array of sentences (strings), of shape (m, 1)
    word_to_index -- a dictionary containing the each word mapped to its index
    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. 
    
    Returns:
    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)
    """
    
    m = X.shape[0]                                   # è®­ç»ƒæ ·æœ¬æ•°é‡
    
    ### START CODE HERE ###
    # å°†X_indicesé›¶åˆå§‹åŒ–ä¸ºæ­£ç¡®shapeçš„numpyçŸ©é˜µ (â‰ˆ 1 line)
    X_indices = np.zeros((m, max_len))
    
    for i in range(m):                               # å¾ªç¯è®­ç»ƒæ ·æœ¬
        
        # å°†ç¬¬iä¸ªè®­ç»ƒå¥å­è½¬æ¢ä¸ºå°å†™ï¼Œå¹¶æ‹†åˆ†ä¸ºå•è¯ã€‚ä¼šå¾—åˆ°ä¸€ä¸ªå•è¯åˆ—è¡¨ã€‚
        sentence_words = X[i].lower().split()
        
        # åˆå§‹åŒ– j ä¸º 0
        j = 0
        
        # å¾ªç¯ sentence_words çš„ words
        for w in sentence_words:
            # å°†X_indicesçš„ç¬¬(i,j)é¡¹è®¾ç½®ä¸ºæ­£ç¡®å•è¯çš„ç´¢å¼•ã€‚
            X_indices[i, j] = word_to_index[w]
            # è‡ªå¢
            j = j + 1
            
    ### END CODE HERE ###
    
    return X_indices
```

#### æµ‹è¯•

è¿è¡Œä¸‹é¢çš„å•å…ƒæ ¼æ£€æŸ¥` sentences_to_indices() `åšäº†ä»€ä¹ˆï¼Œå¹¶æ£€æŸ¥ç»“æœã€‚

```python
X1 = np.array(["funny lol", "lets play baseball", "food is ready for you"])
X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)
print("X1 =", X1)
print("X1_indices =", X1_indices)
```

#### ç»“æœ

```
X1 = ['funny lol' 'lets play baseball' 'food is ready for you']
X1_indices = [[155345. 225122.      0.      0.      0.]
 [220930. 286375.  69714.      0.      0.]
 [151204. 192973. 302254. 151349. 394475.]]
```

### æ„å»º

è®©æˆ‘ä»¬ä½¿ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„å•è¯å‘é‡åœ¨Kerasä¸­æ„å»º`Embedding()`å±‚ã€‚åœ¨æ„å»ºæ­¤å±‚ä¹‹åï¼Œä½ å°†æŠŠ`sentences_to_indices()`çš„è¾“å‡ºä½œä¸ºè¾“å…¥ä¼ é€’ç»™å®ƒï¼Œè€Œ` Embedding() `å±‚å°†è¿”å›å¥å­çš„embeddingsã€‚

```python
# GRADED FUNCTION: pretrained_embedding_layer

def pretrained_embedding_layer(word_to_vec_map, word_to_index):
    """
    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.
    
    Arguments:
    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.
    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)

    Returns:
    embedding_layer -- pretrained layer Keras instance
    """
    
    vocab_len = len(word_to_index) + 1                  # æ·»åŠ 1æ¥è´´åˆKerasåµŒå…¥ (å¿…éœ€)
    emb_dim = word_to_vec_map["cucumber"].shape[0]      # å®šä¹‰GloVeè¯å‘é‡çš„ç»´æ•° (= 50)
    
    ### START CODE HERE ###
    # é›¶åˆå§‹åŒ–åµŒå…¥çŸ©é˜µï¼Œå…¶shapeä¸º(vocab_len, dimensions of word vectors = emb_dim)
    emb_matrix = np.zeros((vocab_len, emb_dim))
    
    # å°†åµŒå…¥çŸ©é˜µçš„æ¯ä¸€è¡Œâ€œindexâ€è®¾ä¸ºè¯æ±‡è¡¨ä¸­å•è¯ç´¢å¼•çš„å•è¯å‘é‡è¡¨ç¤º
    for word, index in word_to_index.items():
        emb_matrix[index, :] = word_to_vec_map[word]

    # å®šä¹‰KerasåµŒå…¥å±‚ä¸æ­£ç¡®çš„è¾“å‡º/è¾“å…¥å¤§å°ï¼Œä½¿å…¶å¯è®­ç»ƒã€‚ä½¿ç”¨Embedding(...)ã€‚ç¡®ä¿è®¾ç½®trainable=Falseã€‚
    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)
    ### END CODE HERE ###

    # æ„å»ºåµŒå…¥å±‚ï¼Œè¿™æ˜¯åœ¨è®¾ç½®åµŒå…¥å±‚æƒé‡ä¹‹å‰æ‰€å¿…éœ€çš„ã€‚ä¸è¦ä¿®æ”¹â€œNoneâ€ã€‚
    embedding_layer.build((None,))
    
    # å°†åµŒå…¥å±‚çš„æƒé‡è®¾ç½®ä¸ºåµŒå…¥çŸ©é˜µã€‚ä½ çš„å±‚ç°åœ¨æ˜¯é¢„å…ˆè®­ç»ƒå¥½çš„ã€‚
    embedding_layer.set_weights([emb_matrix])
    
    return embedding_layer
```

#### æµ‹è¯•

```python
embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)
print("weights[0][1][3] =", embedding_layer.get_weights()[0][1][3])
```

#### ç»“æœ

```
weights[0][1][3] = -0.3403
```



## æ„å»ºEmojifier-V2

### æ„å»ºEmojifier-V2æ¨¡å‹

ç°åœ¨è®©æˆ‘ä»¬æ„å»ºEmojifier-V2æ¨¡å‹ã€‚æ‚¨å°†ä½¿ç”¨å·²æ„å»ºçš„åµŒå…¥å±‚æ¥å®Œæˆæ­¤æ“ä½œï¼Œå¹¶å°†å…¶è¾“å‡ºæä¾›ç»™LSTMç½‘ç»œã€‚

```python
# GRADED FUNCTION: Emojify_V2

def Emojify_V2(input_shape, word_to_vec_map, word_to_index):
    """
    Function creating the Emojify-v2 model's graph.
    
    Arguments:
    input_shape -- shape of the input, usually (max_len,)
    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation
    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)

    Returns:
    model -- a model instance in Keras
    """
    
    ### START CODE HERE ###
    # å®šä¹‰sentence_indicesä½œä¸ºå›¾çš„è¾“å…¥ï¼Œshape=input_shapeï¼Œdtype='int32'(å› ä¸ºå®ƒåŒ…å«ç´¢å¼•)ã€‚
    sentence_indices = Input(shape=input_shape, dtype='int32')
    
    # ä½¿ç”¨GloVeå‘é‡åˆ›å»ºé¢„å…ˆè®­ç»ƒå¥½çš„åµŒå…¥å±‚ (â‰ˆ1 line)
    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)
    
    # é€šè¿‡ä½ çš„åµŒå…¥å±‚ä¼ æ’­sentence_indicesï¼Œå¾—åˆ°embeddings
    embeddings = embedding_layer(sentence_indices)
    
    # é€šè¿‡å…·æœ‰128ç»´éšè—çŠ¶æ€çš„LSTMå±‚ä¼ æ’­embeddings
    # æ³¨æ„ï¼Œè¿”å›çš„è¾“å‡ºåº”è¯¥æ˜¯ä¸€æ‰¹åºåˆ—ã€‚
    X = LSTM(128, return_sequences=True)(embeddings)
    # åŠ ä¸Šæ¦‚ç‡ä¸º0.5çš„dropout
    X = Dropout(0.5)(X)
    # é€šè¿‡å…·æœ‰128ç»´éšè—çŠ¶æ€çš„LSTMå±‚ä¼ æ’­X
    # æ³¨æ„ï¼Œè¿”å›çš„è¾“å‡ºåº”è¯¥æ˜¯å•ä¸ªéšè—çŠ¶æ€ï¼Œè€Œä¸æ˜¯ä¸€æ‰¹åºåˆ—ã€‚
    X = LSTM(128)(X)
    # åŠ ä¸Šæ¦‚ç‡ä¸º0.5çš„dropout
    X = Dropout(0.5)(X)
    # ä¼ æ’­Xé€šè¿‡ä¸€ä¸ªDenseå±‚ä¸softmaxæ¿€æ´»ï¼Œä»¥è·å¾—ä¸€æ‰¹5ç»´å‘é‡ã€‚
    X = Dense(5)(X)
    # æ·»åŠ softmaxæ¿€æ´»
    X = Activation('softmax')(X)
    
    # åˆ›å»ºå°†sentence_indicesè½¬æ¢ä¸ºXçš„æ¨¡å‹å®ä¾‹ã€‚
    model = Model(inputs=sentence_indices, output=X)
    
    ### END CODE HERE ###
    
    return model
```

#### æµ‹è¯•

è¿è¡Œä»¥ä¸‹å•å…ƒæ ¼ä»¥åˆ›å»ºæ¨¡å‹å¹¶æ£€æŸ¥å…¶æ‘˜è¦ã€‚å› ä¸ºæ•°æ®é›†ä¸­æ‰€æœ‰çš„å¥å­éƒ½å°‘äº10ä¸ªå•è¯ï¼Œæ‰€ä»¥æˆ‘ä»¬é€‰æ‹©` max_len = 10`ã€‚ä½ åº”è¯¥çœ‹åˆ°æ‚¨çš„ä½“ç³»ç»“æ„ï¼Œå®ƒä½¿ç”¨â€œ20,223,927â€å‚æ•°ï¼Œå…¶ä¸­æœ‰20,000,050(å•è¯embeddings)æ˜¯ä¸å¯è®­ç»ƒçš„ï¼Œå…¶ä½™çš„223,877æ˜¯å¯ä»¥çš„ã€‚å› ä¸ºæˆ‘ä»¬çš„è¯æ±‡è¡¨å¤§å°æœ‰400,001ä¸ªå•è¯(æœ‰æ•ˆç´¢å¼•ä»0åˆ°400,000)ï¼Œæ‰€ä»¥æœ‰400,001*50 = 20,000,050ä¸å¯è®­ç»ƒçš„å‚æ•°ã€‚

```python
model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)
model.summary()
```

#### ç»“æœ

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 10)                0         
_________________________________________________________________
embedding_2 (Embedding)      (None, 10, 50)            20000050  
_________________________________________________________________
lstm_1 (LSTM)                (None, 10, 128)           91648     
_________________________________________________________________
dropout_1 (Dropout)          (None, 10, 128)           0         
_________________________________________________________________
lstm_2 (LSTM)                (None, 128)               131584    
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 645       
_________________________________________________________________
activation_1 (Activation)    (None, 5)                 0         
=================================================================
Total params: 20,223,927
Trainable params: 223,877
Non-trainable params: 20,000,050
_________________________________________________________________
```

### ç¼–è¯‘è®­ç»ƒæ¨¡å‹

é€šå¸¸ï¼Œåœ¨Kerasä¸­åˆ›å»ºæ¨¡å‹ä¹‹åï¼Œæ‚¨éœ€è¦ç¼–è¯‘å®ƒå¹¶å®šä¹‰æ‚¨æƒ³è¦ä½¿ç”¨çš„æŸå¤±ã€ä¼˜åŒ–å™¨å’Œåº¦é‡ã€‚ä½¿ç”¨`categorical_crossentropy ` æŸå¤±ï¼Œ` adam ` ä¼˜åŒ–å™¨å’Œ` ['accuracy'] `æŒ‡æ ‡æ¥ç¼–è¯‘ä½ çš„æ¨¡å‹:

```python
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
```

æ˜¯æ—¶å€™è®­ç»ƒä½ çš„æ¨¡å‹äº†ã€‚ Emojifier-V2 `model`ä¸­ï¼š

è¾“å…¥ï¼šshape (`m`, `max_len`)æ•°ç»„

è¾“å‡ºï¼šshape (`m`, `number of classes`)æ¦‚ç‡å‘é‡ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»å°†X_train(å¥å­æ•°ç»„ä½œä¸ºå­—ç¬¦ä¸²)è½¬æ¢ä¸ºX_train_indices(å¥å­æ•°ç»„ä½œä¸ºå•è¯ç´¢å¼•åˆ—è¡¨)ï¼Œå°†Y_train(æ ‡ç­¾ä½œä¸ºç´¢å¼•)è½¬æ¢ä¸ºY_train_oh(æ ‡ç­¾ä½œä¸ºone-hotå‘é‡)ã€‚

```python
X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)
Y_train_oh = convert_to_one_hot(Y_train, C = 5)
```

åœ¨` X_train_indices `å’Œ` Y_train_oh `ä¸Šæ‹ŸåˆKerasæ¨¡å‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨`epochs = 50` å’Œ `batch_size = 32`.

```python
model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)
```

#### ç»“æœ

```
Epoch 1/50
132/132 [==============================] - 2s 17ms/step - loss: 1.6084 - acc: 0.1742
Epoch 2/50
132/132 [==============================] - 0s 1ms/step - loss: 1.5337 - acc: 0.3030
Epoch 3/50
132/132 [==============================] - 0s 1ms/step - loss: 1.5025 - acc: 0.3182
Epoch 4/50
132/132 [==============================] - 0s 1ms/step - loss: 1.4405 - acc: 0.3561
Epoch 5/50
132/132 [==============================] - 0s 1ms/step - loss: 1.3506 - acc: 0.4545
Epoch 6/50
132/132 [==============================] - 0s 1ms/step - loss: 1.2368 - acc: 0.5303
Epoch 7/50
132/132 [==============================] - 0s 1ms/step - loss: 1.1772 - acc: 0.4697
Epoch 8/50
132/132 [==============================] - 0s 970us/step - loss: 1.0549 - acc: 0.5758
Epoch 9/50
132/132 [==============================] - 0s 1ms/step - loss: 0.8770 - acc: 0.7045
Epoch 10/50
132/132 [==============================] - 0s 939us/step - loss: 0.8224 - acc: 0.7045
Epoch 11/50
132/132 [==============================] - 0s 939us/step - loss: 0.7017 - acc: 0.7424
Epoch 12/50
132/132 [==============================] - 0s 939us/step - loss: 0.5992 - acc: 0.7955
Epoch 13/50
132/132 [==============================] - 0s 909us/step - loss: 0.4907 - acc: 0.8333
Epoch 14/50
132/132 [==============================] - 0s 1ms/step - loss: 0.5112 - acc: 0.8333
Epoch 15/50
132/132 [==============================] - 0s 992us/step - loss: 0.4819 - acc: 0.8182
Epoch 16/50
132/132 [==============================] - 0s 1ms/step - loss: 0.3525 - acc: 0.8636
Epoch 17/50
132/132 [==============================] - 0s 970us/step - loss: 0.3909 - acc: 0.8561
Epoch 18/50
132/132 [==============================] - 0s 939us/step - loss: 0.6492 - acc: 0.8182
Epoch 19/50
132/132 [==============================] - 0s 1ms/step - loss: 0.5185 - acc: 0.8106
Epoch 20/50
132/132 [==============================] - 0s 1ms/step - loss: 0.3950 - acc: 0.8409
Epoch 21/50
132/132 [==============================] - 0s 1ms/step - loss: 0.4679 - acc: 0.8182
Epoch 22/50
132/132 [==============================] - 0s 1ms/step - loss: 0.3919 - acc: 0.8636
Epoch 23/50
132/132 [==============================] - 0s 1ms/step - loss: 0.3756 - acc: 0.8561
Epoch 24/50
132/132 [==============================] - 0s 1ms/step - loss: 0.3071 - acc: 0.9091
Epoch 25/50
132/132 [==============================] - 0s 962us/step - loss: 0.3457 - acc: 0.8864
Epoch 26/50
132/132 [==============================] - 0s 795us/step - loss: 0.2453 - acc: 0.9394
Epoch 27/50
132/132 [==============================] - 0s 1ms/step - loss: 0.3159 - acc: 0.8788
Epoch 28/50
132/132 [==============================] - 0s 977us/step - loss: 0.2440 - acc: 0.9318
Epoch 29/50
132/132 [==============================] - 0s 985us/step - loss: 0.3898 - acc: 0.8712
Epoch 30/50
132/132 [==============================] - 0s 985us/step - loss: 0.2634 - acc: 0.9091
Epoch 31/50
132/132 [==============================] - 0s 1ms/step - loss: 0.2933 - acc: 0.8864
Epoch 32/50
132/132 [==============================] - 0s 1ms/step - loss: 0.1983 - acc: 0.9318
Epoch 33/50
132/132 [==============================] - 0s 1ms/step - loss: 0.2088 - acc: 0.9470
Epoch 34/50
132/132 [==============================] - 0s 1ms/step - loss: 0.1604 - acc: 0.9621
Epoch 35/50
132/132 [==============================] - 0s 1ms/step - loss: 0.1653 - acc: 0.9621
Epoch 36/50
132/132 [==============================] - 0s 1ms/step - loss: 0.1929 - acc: 0.9394
Epoch 37/50
132/132 [==============================] - 0s 1ms/step - loss: 0.1983 - acc: 0.9470
Epoch 38/50
132/132 [==============================] - 0s 886us/step - loss: 0.2304 - acc: 0.9318
Epoch 39/50
132/132 [==============================] - 0s 909us/step - loss: 0.1445 - acc: 0.9545
Epoch 40/50
132/132 [==============================] - 0s 1ms/step - loss: 0.1670 - acc: 0.9470
Epoch 41/50
132/132 [==============================] - 0s 1ms/step - loss: 0.0886 - acc: 0.9848
Epoch 42/50
132/132 [==============================] - 0s 1ms/step - loss: 0.0877 - acc: 0.9697
Epoch 43/50
132/132 [==============================] - 0s 1ms/step - loss: 0.0845 - acc: 0.9773
Epoch 44/50
132/132 [==============================] - 0s 758us/step - loss: 0.0519 - acc: 0.9924
Epoch 45/50
132/132 [==============================] - 0s 795us/step - loss: 0.0753 - acc: 0.9848
Epoch 46/50
132/132 [==============================] - 0s 1ms/step - loss: 0.0811 - acc: 0.9697
Epoch 47/50
132/132 [==============================] - 0s 970us/step - loss: 0.1190 - acc: 0.9545
Epoch 48/50
132/132 [==============================] - 0s 947us/step - loss: 0.2948 - acc: 0.9167
Epoch 49/50
132/132 [==============================] - 0s 1ms/step - loss: 0.0945 - acc: 0.9773
Epoch 50/50
132/132 [==============================] - 0s 1ms/step - loss: 0.0784 - acc: 0.9773
```

### è¯„ä¼°

æ‚¨çš„æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„ç²¾åº¦åº”è¯¥æ¥è¿‘**100% **ã€‚æ‚¨å¾—åˆ°çš„ç²¾ç¡®ç²¾åº¦å¯èƒ½ç•¥æœ‰ä¸åŒã€‚è¿è¡Œä»¥ä¸‹å•å…ƒæ¥è¯„ä¼°æµ‹è¯•é›†ä¸Šçš„æ¨¡å‹ã€‚

```python
X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)
Y_test_oh = convert_to_one_hot(Y_test, C = 5)
loss, acc = model.evaluate(X_test_indices, Y_test_oh)
print()
print("Test accuracy = ", acc)
```

#### ç»“æœ

```
56/56 [==============================] - 0s 286us/step

Test accuracy =  0.8035714200564793
```

### æŸ¥çœ‹è¯¯å·®

æµ‹è¯•å‡†ç¡®åº¦åº”è¯¥åœ¨80%åˆ°95%ä¹‹é—´ã€‚è¿è¡Œä¸‹é¢çš„å•å…ƒæ ¼å¯ä»¥çœ‹åˆ°æ ‡è®°é”™è¯¯çš„ç¤ºä¾‹ã€‚

```python
# è¿™æ®µä»£ç å…è®¸æ‚¨æŸ¥çœ‹æ ‡è®°é”™è¯¯çš„ç¤ºä¾‹
C = 5
y_test_oh = np.eye(C)[Y_test.reshape(-1)]
X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)
pred = model.predict(X_test_indices)
for i in range(len(X_test)):
    x = X_test_indices
    num = np.argmax(pred[i])
    if(num != Y_test[i]):
        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())
```

#### ç»“æœ

```
Expected emoji:ğŸ˜„ prediction: she got me a nice present	â¤ï¸
Expected emoji:ğŸ˜ prediction: work is hard	ğŸ˜„
Expected emoji:ğŸ˜ prediction: This girl is messing with me	â¤ï¸
Expected emoji:ğŸ´ prediction: any suggestions for dinner	ğŸ˜„
Expected emoji:â¤ï¸ prediction: I love taking breaks	ğŸ˜
Expected emoji:ğŸ˜„ prediction: you brighten my day	â¤ï¸
Expected emoji:ğŸ˜ prediction: she is a bully	â¤ï¸
Expected emoji:ğŸ˜„ prediction: will you be my valentine	â¤ï¸
Expected emoji:ğŸ´ prediction: See you at the restaurant	â¤ï¸
Expected emoji:ğŸ˜ prediction: go away	âš¾
Expected emoji:ğŸ´ prediction: I did not have breakfast â¤ï¸
```

ç°åœ¨æ‚¨å¯ä»¥åœ¨è‡ªå·±çš„ç¤ºä¾‹ä¸­å°è¯•å®ƒã€‚åœ¨ä¸‹é¢å†™ä¸‹ä½ è‡ªå·±çš„å¥å­ã€‚

```python
# æ”¹å˜ä¸‹é¢çš„å¥å­çœ‹çœ‹ä½ çš„é¢„æµ‹ã€‚ç¡®ä¿æ‰€æœ‰çš„å•è¯éƒ½åœ¨GloVeåµŒå…¥å¤„ã€‚ 
x_test = np.array(['not feeling happy'])
X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)
print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))
not feeling happy ğŸ˜
```

ä¹‹å‰ï¼ŒEmojify-V1æ¨¡å‹å¹¶æ²¡æœ‰æ­£ç¡®åœ°æ ‡æ³¨â€œnot feeling happyâ€ï¼Œä½†æ˜¯æˆ‘ä»¬å¯¹Emojiy-V2çš„å®ç°åšåˆ°äº†è¿™ä¸€ç‚¹ã€‚(Kerasçš„è¾“å‡ºæ¯æ¬¡éƒ½æœ‰ä¸€ç‚¹éšæœºæ€§ï¼Œæ‰€ä»¥å¯èƒ½ä¸ä¼šå¾—åˆ°å®Œå…¨ç›¸åŒçš„ç»“æœã€‚)ç›®å‰çš„æ¨¡å‹åœ¨ç†è§£å¦å®š(æ¯”å¦‚â€œnot happyâ€)æ–¹é¢è¿˜ä¸æ˜¯å¾ˆå¥å…¨ï¼Œå› ä¸ºè®­ç»ƒé›†å¾ˆå°ï¼Œæ‰€ä»¥æ²¡æœ‰å¤ªå¤šçš„å¦å®šä¾‹å­ã€‚ä½†æ˜¯å¦‚æœè®­ç»ƒé›†æ›´å¤§ï¼ŒLSTMæ¨¡å‹åœ¨ç†è§£è¿™æ ·å¤æ‚çš„å¥å­æ–¹é¢è¦æ¯”Emojify-V1æ¨¡å‹å¥½å¾—å¤šã€‚

### æ­å–œ!

æ­å–œä½ å®Œæˆäº†ä»»åŠ¡! â¤ï¸â¤ï¸â¤ï¸

**ä½ åº”è¯¥è®°ä½çš„**:

- å¦‚æœä½ æœ‰ä¸€ä¸ªè®­ç»ƒé›†å¾ˆå°çš„NLPä»»åŠ¡ï¼Œä½¿ç”¨è¯åµŒå…¥å¯ä»¥æ˜¾è‘—å¸®åŠ©ä½ çš„ç®—æ³•ã€‚è¯åµŒå…¥çš„è®©ä½ çš„æ¨¡å‹åœ¨æµ‹è¯•é›†èƒ½å¤Ÿå¤„ç†ç”šè‡³ä¸å‡ºç°åœ¨ä½ çš„è®­ç»ƒé›†çš„å•è¯ã€‚è®­ç»ƒåºåˆ—æ¨¡å‹Keras(å’Œå¤§å¤šæ•°å…¶ä»–æ·±åº¦å­¦ä¹ æ¡†æ¶)éœ€è¦ä¸€äº›é‡è¦çš„ç»†èŠ‚:
	- ä½¿ç”¨mini-batchesåºåˆ—éœ€è¦å¡«å……,è¿™æ ·æ‰€æœ‰çš„ä¾‹å­mini-batch**å…·æœ‰ç›¸åŒçš„é•¿åº¦**ã€‚
- `Embedding()`å±‚å¯ä»¥ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„å€¼è¿›è¡Œåˆå§‹åŒ–ã€‚è¿™äº›å€¼å¯ä»¥æ˜¯å›ºå®šçš„ï¼Œä¹Ÿå¯ä»¥åœ¨æ•°æ®é›†ä¸Šè¿›ä¸€æ­¥è®­ç»ƒã€‚ä½†æ˜¯ï¼Œå¦‚æœæ‚¨çš„æ ‡è®°æ•°æ®é›†å¾ˆå°ï¼Œé‚£ä¹ˆé€šå¸¸ä¸å€¼å¾—å°è¯•è®­ç»ƒå¤§é‡é¢„å…ˆè®­ç»ƒè¿‡çš„åµŒå…¥é›†ã€‚` LSTM() `æœ‰ä¸€ä¸ªåä¸º`return_sequences `çš„æ ‡å¿—æ¥å†³å®šä½ æ˜¯æƒ³è¿”å›æ¯ä¸ªéšè—çŠ¶æ€è¿˜æ˜¯åªè¿”å›æœ€åä¸€ä¸ªã€‚
- æ‚¨å¯ä»¥åœ¨` LSTM() `ä¹‹åä½¿ç”¨` Dropout() `æ¥è§„èŒƒæ‚¨çš„ç½‘ç»œã€‚

ç¥è´ºä½ å®Œæˆäº†è¿™é¡¹ä»»åŠ¡ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªè¡¨æƒ…ç¬¦å·!

# ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€
