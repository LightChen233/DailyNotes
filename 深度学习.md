# **深度学习**

# 神经网络与深度学习

## 概论

### 深度学习是什么？

深度学习指的是训练很大的神经网络

### 神经网络是什么？

房价预测模型

假设有6间房屋的数据集（房屋面积s已知,价格已知）

想要找到一个函数能够很好地预测房价的趋势 

如果你懂得线性回归，就可以拟合一条曲线

![image-20200805191119034](.\img\image-20200805191119034.png)

由于房价永远不为负，所以纯粹的直线不大合适

![image-20200805191715321](.\img\image-20200805191715321.png)

这个房屋加上拟合曲线，看城市一个非常简单的神经网络。

#### 神经元

我们把房屋的面积，作为神经网络的输入x，通过一个节点，最后输出了价格y，这个节点就是独立的**神经元**

```mermaid
graph LR
A[房屋面积x] -->B(神经元)-->C[价格y]

```

这个神经元所做的就是输入面积，进行线性运算，取不小于0的值最后得到输出预测价格。

#### ReLU函数

形如上文的这种函数，被称为ReLU函数（修正线性单元）rectified linear unit。修正指的是取不小于0的值

#### 神经网络

把这些单个的神经元堆叠，就形成了一张网络，及神经网络

同样的，如果不仅仅是房屋的面积，我们还知道一些其他的特征影响房价信息。比如卧室数量（影响家庭人口数量），邮编（影响高度步行化，学校质量），富裕程度（影响学校质量）
```mermaid
graph LR

房屋面积-->A[x1] -->E(家庭成员多少)
卧室数量-->B[x2]-->E
邮政编码-->C[x3]-->F(高度步行化)
富裕程度-->D[x4]-->G(学校质量)
	C-->G
	E-->H(节点)
	F-->H
	G-->H
	H-->I[价格y]

```
```mermaid
graph LR

房屋面积-.-A[x1] -->E( )
卧室数量-.-B[x2]-->E
邮政编码-.-C[x3]-->F( )
富裕程度-.-D[x4]-->G( )
	A-->F
	A-->G
	B-->F
	B-->G
	C-->E
	C-->G
	D-->E
	D-->F
	E-->H( )
	F-->H
	G-->H
	H-->I[价格y]

```

$x_i$为输入层， 中间三个圈称为隐藏单元，每个输入都同时来自四个特征（中间连接数很高）需要给予足够的x，y训练样本，神经网络就能计算从x到y的精准映射函数

### 监督学习

```mermaid
graph LR

A[X]==>|神经元|B[Y]

```
| 输入(x)   | 输出(y)        | 应用         |                                    |
| --------- | -------------- | ------------ | ---------------------------------- |
| 房产状况  | 价格           | 预测评估房价 | Standard NN                        |
| 用户信息  | 是否推送广告   | 在线广告     | Standard NN                        |
| 图像      | 判断物体       | 图像识别     | CNN                                |
| 音频      | 文本           | 语音识别     | RNN                                |
| 英语      | 中文           | 机器翻译     | RNNs                               |
| 图像+雷达 | 与其他车的距离 | 自动驾驶     | Hybrid neural network architecture |

对于序列数据，比如音频中的时间元素，音频是随着时间播放的，所以音频很自然的被表示为一维时间序列，对于序列数据，经常使用RNN循环神经网络。语言英语汉语字母或单词都是逐个出现的。所以语言最自然的表示方式也是序列数据。更为复杂的RNNs往往会应用这些方面。

更加复杂的如无人驾驶等等，包含图像(经常使用CNN卷积神经网络架构处理)，雷达（其他类型）等信息会使用更加复杂的混合神经网络结构

#### 图例

##### Standard NN

<img src=".\img\image-20200805193759614.png" alt="image-20200805193759614" style="zoom:50%;" />

##### CNN

<img src=".\img\image-20200805193812718.png" alt="image-20200805193812718" style="zoom:50%;" />

##### RNN

<img src=".\img\image-20200805193826014.png" alt="image-20200805193826014" style="zoom:50%;" />

#### 结构化数据

有一定结构存放的数据

![image-20200805194009905](.\img\image-20200805194009905.png)

#### 非结构化数据

例如图片，音频等信息数据

![image-20200805194027429](.\img\image-20200805194027429.png)

多亏机器学习和深度学习，计算机现在能够更好的解释非结构化数据。同时也创造出了大量的应用场景，如语音识别，图像识别，自然语言文字处理。神经网络在很多短期经济价值的创造是基于结构化数据的，比如更好的广告，更好的获利建议。所以对于结构化数据，神经网络有更好的能力去处理海量的数据库。

### 深度学习的性能

#### 数据规模驱动

![image-20200805201335275](.\img/image-20200805201335275.png)

#### 计算性能提升

#### 算法改进

使卷积神经网络更快。

例如对于sigmoid函数，在两段倒数会无限趋近于0，学习效率会大大减慢，通过改变激活函数，将其优化成ReLU，使梯度下降法更快

![image-20200805201828180](.\img/image-20200805201828180.png)

#### 实践过程

<img src="./img/image-20200805202132726.png" alt="image-20200805202132726" style="zoom:50%;" />

## 神经网络基础

### 二分分类

<table> 
    <tr>
        <td>
        **Model**
        </td>
        <td>
        **Train accuracy**
        </td>
        <td>
        **Problem/Comment**
        </td>
    </tr>
    <tr>
        <td>
        3-layer NN with zeros initialization
        </td>
        <td>
        50%
        </td>
        <td>
        fails to break symmetry
        </td>
    </tr>
    <tr>
        <td>
        3-layer NN with large random initialization
        </td>
        <td>
        83%
        </td>
        <td>
        too large weights 
        </td>
    </tr>
    <tr>
        <td>
        3-layer NN with He initialization
        </td>
        <td>
        99%
        </td>
        <td>
        recommended method
        </td>
    </tr>
</table> 

#### 引子

比如有这样一张图

![image-20200805203242265](./img//image-20200805203242265.png)

如果这张图判定为有小猫则输出1，否则输出0，我们用y来表示输出结果标签。

首先，图像在计算机中是由RGB三种颜色组成的。如果该图为64×64像素大小，那么就有三个64×64的矩阵。对应红绿蓝三种颜色的亮度。

![image-20200805203740125](./img//image-20200805203740125.png)

把里面所有元素（即像素值）都提取出来放入一个特征向量x，那么结果就会是这样：

![image-20200805203952919](./img//image-20200805203952919.png)

#### 将来用到的符号

用一对(x,y)来表示一个单独的样本，x是$n_x$维的特征向量，y值为0或1。训练集由m个训练样本构成。

$（x^{(1)},y^{(1)})$表示样本1的输入与输出，$（x^{(2)},y^{(2)})$表示样本2的输入与输出.....$（x^{(m)},y^{(m)})$表示样本m的输入与输出

m为训练集总数，$m=m_{train}$，$m_{test}$表示测试集的数量。

最后用更紧凑的符号表示训练集

$$
X=\left[
 \begin{matrix}
   | & | & | & | & |\\
   | & | & | & | & | \\
   x_1 & x_2 & * & * & x_m \\
   | & | & | & | & |
  \end{matrix} 
\right]_{m×n_x}
$$

$$
Y=\left[
 \begin{matrix}
   y_1 & y_2 & * & * & y_m\\
  \end{matrix} 
\right]_{1×m}
$$

### logistic回归

已知:	x，

要求:	$\hat{y}=P(y=1|x)$

参数:	X为$n_x$维向量($x∈R^{n_x}$)，回归参数$w∈R^{n_x}$,$b∈R$

输出:	$\hat{y}=w^Tx+b$

但是因为希望$\hat{y}$为y=1的概率，所以$\hat{y}$应介于0,1之间。

所以$\hat{y}=σ(w^Tx+b)$【取sigmoid函数】。

$σ(z)=\frac{1}{1+e^{-z}}$

<img src="./img/image-20200805212109031.png" alt="image-20200805212109031" style="zoom:50%;" />



为了训练logistic回归模型的参数w以及b，需要定义一个成本函数。

$\hat{y}$是对一个训练样本x来说，对于每个训练样本使用这些带有圆括号的上标。方便应用说明，还有区分样本，你的训练样本(i)对应的预测值是$\hat{y}^{(i)}$是用训练样本通过sigmoid函数作用到$w^Tx+b$得到的。

#### 损失函数/误差函数

可以用来衡量算法的运行情况。我们可以定义损失为$L(\hat{y},y)=\frac{(\hat{y}-y)^2}{2}$，但是大家一般不会这样做，因为之后讨论的优化问题会变成非凸的，最后会得到很多个局部最优解，梯度下降法可能找不到全局最优值。

因此，为了能够达到平方相似的目的，误差平方越小越好，我们定义损失函数为：
$$
L(\hat{y},y)=-(ylog\hat{y}+(1-y)log(1-\hat{y}))
$$
当y=1时：$L(\hat{y},y)=-log\hat{y}$ 需要$\hat{y}$足够的大，但$\hat{y}<1$,所以$\hat{y}->1$

当y=0时：$L(\hat{y},y)=-log(1-\hat{y})$ 需要$\hat{y}$足够的小，但$\hat{y}>0$,所以$\hat{y}->0$.

损失函数是在单个训练样本中定义的，它衡量了在单个训练样本上的表现。

#### 成本函数（Cost Function）

它衡量的是在全体训练样本上的表现

$$
J(w,b)=\frac{\sum { L(\hat{y}^{(i)},y^{(i)})}}{m}=-\frac{\sum { y^{(i)}log\hat{y}^{(i)}+(1-y^{(i)})log(1-\hat{y}^{(i)})}}{m}
$$

损失函数只适用于单个训练样本，成本函数基于参数总成本，所以在训练logistic回归模型时，我们要找到合适的w和b是这里的成本函数J尽可能地小

#### 梯度下降法

用梯度下降法训练或学习训练集上的参数w和b。

J(w,b)是在水平轴w和b上的曲面，其高度代表了J(w,b)在某一点的值。

<img src="./img/image-20200806141336155.png" alt="image-20200806141336155" style="zoom:50%;" />

如图，J是这样的一个凸函数。用某个初始值，初始化w和b。对于logistic回归而言，几乎任意的初始化方案都有效，通常用0。但是对于logistic回归，我们通常不这么做。但由于该函数是凸的，无论哪里初始化，都应该达到统一点火大致相同的点。梯度下降法所做的就是，从初始点开始，朝最陡的下坡方向一步步往下走，并很有希望收敛到这个（接近）全局最优解。

<img src="./img/image-20200806142013867.png" alt="image-20200806142013867" style="zoom:50%;" />

希望得到最小化J(w)，为了简化，首先忽略b，仅用一维曲线，代替多维曲线。

<img src="./img/image-20200806142907531.png" style="zoom:30%;" />

梯度下降法是这样做的：

重复执行以下的更新操作：
$$
w=w-\alpha\frac{dJ(w)}{dw}
$$


$\alpha$表示学习率，可以控制每一次迭代，可以控制每一次迭代，或者梯度下降法中的步长。之后会讨论如何选择$\alpha$。其次，在这里，这个数是导数，这就是对参数w的更新，或者变化量。代码中，我们会用dw表示导数。即:


$$
w=w-\alpha dw
$$


<img src="./img/image-20200806143906702.png" alt="image-20200806143906702" style="zoom:30%;" />

根据公式，该函数反复更新，会逐渐的接近最低点。


$$
w=w-\alpha\frac{dJ(w,b)}{dw}\\
b=b-\alpha\frac{dJ(w,b)}{db}
$$
例如：

$$J(a,b,c)=3(a+bc)$$当a=5,b=3,c=2时，

#### 正向传播

```mermaid
graph LR

a=5-->A[v=a+u=11]
b=3-->B[u=bc=6]
c=2-->B
B-->A
A-->J=3v=33

```
#### 反向传播

```mermaid
graph RL

A[v=a+u]-->C[a=5]
B[u=bc]-->b=3
B-->c=2
A-->B
D[J=3v]-->A
dv=dJ/dv=3-.-A
da=dJ/da=3-.-C
du=dJ/du=3-.-B
```

#### 最小化损失函数

<img src="./img/image-20200806211521197.png" alt="image-20200806211521197" style="zoom:50%;" />

![./image-20200806153252466](img/image-20200806153252466.png)

### 向量化

#### 什么是向量化？

在logistic回归中你需要去计算$z=w^Tx+b$，w是列向量，x也是列向量。如果有很多特征，他们就是非常大的向量，所以w和x都是R内的$n_x$维的向量。所以去计算W'X需要向量化，后会加快其速度

![](./img/QQ截图20200806153858.jpg)

![](./img/QQ截图20200806154718.jpg)

可以看到向量化速度远远快于for，所以一定要尽量避免for循环

![](./img/QQ截图20200806155105.jpg)

所以以后使用for前先看看有没有numpy的内置方法

![](img/QQ截图20200806155247.jpg)

![](img/QQ截图20200806155442.jpg)

![](img/QQ截图20200806155922.jpg)

![](img/QQ截图20200806161240.jpg)

![](img/QQ截图20200806161357.jpg)

![](img/QQ截图20200806161748.jpg)

![](img/QQ截图20200806161908.jpg)

#### Python中的广播

![](img/QQ截图20200806162241.jpg)

优点：极其方便快捷

缺点：可能会产生一些奇怪的内在逻辑错误。

![](img/QQ截图20200806163214.jpg)

尽量少用形如（n,）的这种秩为1的数组，使用(n,1)确保不会出现a*a.T不会出现问题

![](img/QQ截图20200806201753.jpg)

输入层  隐藏层 输出层

![](img/QQ截图20200806202013.jpg)

![](img/QQ截图20200807132216.jpg)

隐藏层有两个相关参数$W^{[1]}_{4×3}，B^{[1]}_{4×1}$

输出层也有类似的相关参数$W^{[2]}_{1×4}【隐藏层有四个隐藏单元】，B^{[2]}_{1×1}$

#### 计算过程

![](img/QQ截图20200807133035.jpg)

![](img/QQ截图20200807133351.jpg)

![]()![QQ截图20200807133525](img/QQ截图20200807133525.jpg)

![](img/QQ截图20200807133931.jpg)

![](img/QQ截图20200807134648.jpg)

![](img/QQ截图20200807143157.jpg)

![](img/QQ截图20200807143335.jpg)

## 浅层神经网络

### 激活函数

#### tanh函数

上文有说明为什么sigmoid函数（σ函数）效果不佳

在(-1,1)范围内$tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$比σ函数有着更好的效果，而且还有一种类似于数据中心化的效果，使得数据的平均值接近0，实际上让下一层的学习更加方便。（几乎在所有场合都更优越）。一个例外是输出层。因为如果y是0或1，那么你希望$\hat y∈(0,1)$之间更合理。而不是$(-1,1)$。所以往往只在二元分类(二元决策问题)用σ激活函数，这里将该函数作为输出层的函数。

不过这两个函数都有一个缺点：当z特别大或者特别小的时候，这个函数的斜率可能很小，趋于0。

所以在机器学习中有个很有趣的玩具，即ReLU函数，公式为a=max(0,z)。所谓的修正线性单元。现在已经变成激活函数的默认选择了。【不知道用啥时，就用它就ok】。唯一的缺点是当z<0时导数等于0。所以，他有个一种改进叫做带泄露ReLU，在z<0时，有一个很平缓的斜率

![](img/QQ截图20200807144035.jpg)

<img src="./img/image-20200807144929773.png" alt="image-20200807144929773" style="zoom:50%;" />

![](img/QQ截图20200807150015.jpg)

![](img/QQ截图20200807150124.jpg)

### 为什么必须使用非线性激活函数？

如果直接相等，那么输出不过是输入特征的线性组合

![](img/QQ截图20200807150433.jpg)

这样不如直接去掉隐藏层，因为不管多少层网络都得到的只能是线性函数，无法得到非线性的。只有一个地方可以使用线性激活函数，如果g(z)=z就是你要机器学习的是回归问题。所以y是一个实数，比如说你想预测房地产价格，而y不是0和1，而是一个实数。这样线性激活函数也许是可行的，你的输入输出也都属于R。所以通常只能在输出层使用。

![](img/QQ截图20200807150720.jpg)

![](img/QQ截图20200807152339.jpg)

![](img/QQ截图20200807152417.jpg)

![](img/QQ截图20200807152439.jpg)

![](img/QQ截图20200807153638.jpg)

![](img/QQ截图20200807153643.jpg)

![](img/QQ截图20200807173135.jpg)

![](img/QQ截图20200807173142.jpg)

![](img/QQ截图20200807173149.jpg)

### 初始化

#### 法一：全部置0

$$
W^{[1]}=\left[
 \begin{matrix}
   0 & 0 \\
   0 & 0 \\
  \end{matrix} 
\right]
$$

给网络输入任何样本，$a^{[1]}_1$和$a^{[1]}_2$是一样的，即两个激活函数完全一样，对于反向传播，事实证明，处于对称性，$dz^{[1]}_1$和$dz^{[1]}_2$也是相同的。所以，
$$
W^{[2]}=\left[
 \begin{matrix}
   0 & 0 \\
  \end{matrix} 
\right]
$$

$a^{[1]}_1$和$a^{[1]}_2$节点计算完全一样的函数，称之为完全对称。


$$
dW=\left[
 \begin{matrix}
   u & v \\
   u & v \\
  \end{matrix} 
\right]
$$

$W^{[1]}=W^{[1]}-\alpha dW$由数学归纳反复迭代可以证明是完全对称的。不管梯度如何下降，都在计算完全一样的函数。

<img src="img/QQ截图20200807175533.jpg" style="zoom:33%;" />

#### 法二：随机初始化

这个问题的解决方案就是随机初始化所有参数。

我们可以令$W^{[1]}=np.random.randn((2,2))*0.01$

```python
rd=np.random.randn((2,2))#产生参数为(2,2)矩阵的高斯分布随机变量。
W1=rd*0.01#将权重初始化成很小的随机数,由于是用sigmoid或tanh激活函数，如果权重太大，就会跑到导数趋于0的点，就会很慢。更深的深度学习时可能需要0.01以外的常数
b1=np.zeros((2,1))#b1不影响对称性，所以置零即可
W2=np.random.randn((1,2))*0.01
b2=0
```

## 深层神经网络

<img src="img/QQ截图20200808130618.jpg" style="zoom:30%;" />

![](img/QQ截图20200808130905.jpg)

![](img/QQ截图20200808131607.jpg)

### Debug技巧

判断向量维数

![image-20200808132800595](img/image-20200808132800595.png)

![image-20200808132827388](img/image-20200808132827388.png)

### 为什么要有深层神经网络

首先，深度学习网络在干什么？

如果你在建一个人脸识别或是人脸检测系统，神经网络做的就是，当你输入一张图片，你可以把第一层神经网络当成一个特征探测器，或者边缘探测器。【边缘探测器其实相对来说都是针对照片中非常小的面积。】

例子中有20个隐藏单元。把这些探测到的边缘组合到一起就可以开始检测人脸的不同部分。比如说有一个神经元专门找眼睛的部分，另外还有别的在找鼻子的部分。最后把所有器官拼在一起，比如鼻子眼睛下巴，就可以识别或是探测不同的人脸了。

![](img/QQ截图20200808160112.jpg)

这种从简单到复杂的金字塔状表示方法或特征方法也可以应用在图像或者人脸识别以外的其他数据上。

比如说你想要的搭建一个语音识别系统的时候，需要解决的就是如何可视化语音，比如你输入一个音频片段，那么神经网络的第一层就会去先开始试着探测**较低层次**的音频波形的一些特征，比如音调高低啊，白噪音啊，嘶嘶嘶的声音等等。可以选择这些相对程度比较低的波性特征，然后把这些波形组合在一起，就能去探测声音的基本单元，即音位。在组合起来，你就能识别音频当中的单词，再到词组，再到句子。**把简单的特征一步步组合成更加复杂的东西。**

### 关于神经网络为何有效的理论

来源于电路理论，可以用相对较小但很深的神经网络来计算。小在这里是指隐藏单元的数量相对较小。

![](img/QQ截图20200808161231.jpg)

### 搭建神经网络块

![](img/QQ截图20200808161423.jpg)

![](img/QQ截图20200808161427.jpg)

![](img/QQ截图20200808145738.jpg)

![](img/QQ截图20200808145754.jpg)

![](img/QQ截图20200808145813.jpg)

### 参数/超参数

为了使深度学习获得更好的效果，我们还需要规划好参数。

除了W和B还有其他参数，需要输入到学习算法中。比如，学习率$\alpha$来决定我们的参数如何进化。

或者还有，梯度下降法循环的数量。

隐层数L,隐藏单元数，选择激活函数等等。

以上这些因素都在某种程度上决定了W和B的值，称为超参数。还有momentum，mini batch的大小，几种不同的正则化参数等等。

这是一个经验化的过程，可以不断尝试，看看损失函数的效果

![](img/QQ截图20200808151021.jpg)

![](img/QQ截图20200808151719.jpg)